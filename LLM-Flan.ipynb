{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c38af97-eb20-49e7-869a-43cfbe5052b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcd1a61b-6fad-4e4e-b8f9-c063eb23e8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardware check: Running on CUDA\n",
      "\n",
      "Loading google/flan-t5-small...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nil\\anaconda3\\envs\\transformers-env\\lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nil\\anaconda3\\envs\\transformers-env\\lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Translate to German: I am successfully running AI on my GPU!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nil\\anaconda3\\envs\\transformers-env\\lib\\site-packages\\transformers\\generation\\utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ich habe erfolgreiches AI auf meinem GPU!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# 1. Verify we are using your Quadro M1200\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Hardware check: Running on {device.upper()}\\n\")\n",
    "\n",
    "# 2. Download and load the FLAN model and tokenizer\n",
    "model_name = \"google/flan-t5-small\"\n",
    "print(f\"Loading {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# 3. Give it a test prompt\n",
    "prompt = \"Translate to German: I am successfully running AI on my GPU!\"\n",
    "print(f\"\\nPrompt: {prompt}\")\n",
    "\n",
    "# 4. Generate the response\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aba5969a-1e74-463c-9808-74f3c11ec989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
      "        num_rows: 12460\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
      "        num_rows: 1500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"knkarthick/dialogsum\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad5a80b4-8d56-4aef-b57d-ec53446d9269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{dataset['test'][40]['dialogue']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c31efc0b-23dd-4050-ad29-08c2d186f3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hardware to be used to run the model is: CUDA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the system and load the dataset\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"The hardware to be used to run the model is: {device.upper()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25a47079-879c-4388-b781-a5fc00c2fba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nil\\anaconda3\\envs\\transformers-env\\lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# download the model and tokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6c79629-eea0-4d15-8a08-2f52cffb5dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt given to the model: Translate to Hindi: I am running large language model on my system\n",
      "The response of model :          \n"
     ]
    }
   ],
   "source": [
    "# testing the model\n",
    "prompt = \"Translate to Hindi: I am running large language model on my system\"\n",
    "print(f\"\\nPrompt given to the model: {prompt}\")\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"The response of model : {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4270f39a-e95a-4554-9942-333f119fa4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input to my model: Give me the sentiment for #Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "The sentiment analysis by the model for the conversation is: negative\n"
     ]
    }
   ],
   "source": [
    "dialogue_text = dataset['test'][40]['dialogue']\n",
    "prompt = f\"Give me the sentiment for {dialogue_text}\"\n",
    "print(f\"The input to my model: {prompt}\\n\")\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"The sentiment analysis by the model for the conversation is: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93f4ad3f-655c-4898-a526-00f2dbed861d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The human written answer for the input to LLM is: #Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "The analysis of the model is: It's ten to nine by my watch.\n"
     ]
    }
   ],
   "source": [
    "dialogue_text = dataset['test'][40]['dialogue']\n",
    "human_summary = dataset['test'][40]['summary']\n",
    "print(f\"The human written summary for the input to LLM is: {human_summary}\")\n",
    "prompt = f\"Summarize the coversation: {dialogue_text}\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"The analysis of the model is: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7757a163-afc6-4e02-8d52-7c4368c7bb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The example dialogue: #Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "The human summary: #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "--- One-Shot Model Summary ---\n",
      "It's ten to nine by my watch.\n"
     ]
    }
   ],
   "source": [
    "example_dialogue = dataset['test'][200]['dialogue'] \n",
    "example_summary = dataset['test'][200]['summary']\n",
    "target_dialogue = dataset['test'][40]['dialogue']\n",
    "print(f\"The example dialogue: {example_dialogue}\")\n",
    "print(f\"The human summary: {example_summary}\")\n",
    "one_shot_prompt = f\"\"\"Summarize this dialogue:\n",
    "{example_dialogue}\n",
    "Summary: {example_summary}\n",
    "\n",
    "Summarize this dialogue:\n",
    "{target_dialogue}\n",
    "Summary:\"\"\"\n",
    "# passing to the model\n",
    "inputs = tokenizer(one_shot_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens = 100)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"--- One-Shot Model Summary ---\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bafb7d-c423-4fb9-ad6a-f1b0719f0bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (transformers-env)",
   "language": "python",
   "name": "transformers-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
